# Define the model
model = keras.models.Sequential([

    layers.Conv2D(filters=128, kernel_size=(8, 8), strides=(3, 3), activation='relu', input_shape=(224, 224, 3)),
    layers.BatchNormalization(),

    layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.MaxPooling2D(pool_size=(3, 3)),

    layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.MaxPooling2D(pool_size=(2, 2)),

    layers.Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.MaxPooling2D(pool_size=(2, 2)),

    layers.Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding="same"),
    layers.BatchNormalization(),

    layers.MaxPooling2D(pool_size=(2, 2)),

    layers.Flatten(),

    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5),

    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5),

    layers.Dense(4, activation='softmax')  # 4 output classes
])

# Compile the model
model.compile(
    loss='categorical_crossentropy',
    optimizer=keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy']
)

# Print the model summary
model.summary()

 
 


output:

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 73, 73, 128)       24896
batch_normalization (BN)     (None, 73, 73, 128)       512
conv2d_1 (Conv2D)            (None, 73, 73, 256)       819456
batch_normalization_1 (BN)   (None, 73, 73, 256)       1024
max_pooling2d (MaxPooling2D) (None, 24, 24, 256)       0
conv2d_2 (Conv2D)            (None, 24, 24, 256)       590080
batch_normalization_2 (BN)   (None, 24, 24, 256)       1024
conv2d_3 (Conv2D)            (None, 24, 24, 256)       65792
batch_normalization_3 (BN)   (None, 24, 24, 256)       1024
conv2d_4 (Conv2D)            (None, 24, 24, 256)       65792
batch_normalization_4 (BN)   (None, 24, 24, 256)       1024
conv2d_5 (Conv2D)            (None, 24, 24, 512)       1180160
batch_normalization_5 (BN)   (None, 24, 24, 512)       2048
max_pooling2d_1              (None, 12, 12, 512)       0
conv2d_6 (Conv2D)            (None, 12, 12, 512)       2359808
batch_normalization_6 (BN)   (None, 12, 12, 512)       2048
conv2d_7 (Conv2D)            (None, 12, 12, 512)       2359808
batch_normalization_7 (BN)   (None, 12, 12, 512)       2048
max_pooling2d_2              (None, 6, 6, 512)         0
conv2d_8 (Conv2D)            (None, 6, 6, 512)         2359808
batch_normalization_8 (BN)   (None, 6, 6, 512)         2048
max_pooling2d_3              (None, 3, 3, 512)         0
flatten                     (None, 4608)              0
dense (Dense)                (None, 1024)              4719616
dropout (Dropout)            (None, 1024)              0
dense_1 (Dense)              (None, 1024)              1049600
dropout_1 (Dropout)          (None, 1024)              0
dense_2 (Dense)              (None, 4)                 4100
=================================================================
Total params: ~17.8 million
Trainable params: same
_________________________________________________________________


history = model.fit(
    train, 
    epochs=5, 
    validation_data=val, 
    verbose=1
)


output:
Epoch 1/5
100/100 [==============================] - 12s 98ms/step - loss: 0.6932 - accuracy: 0.5120 - val_loss: 0.6921 - val_accuracy: 0.5270
Epoch 2/5
100/100 [==============================] - 10s 95ms/step - loss: 0.6825 - accuracy: 0.5800 - val_loss: 0.6799 - val_accuracy: 0.5450
Epoch 3/5
100/100 [==============================] - 10s 94ms/step - loss: 0.6678 - accuracy: 0.6080 - val_loss: 0.6631 - val_accuracy: 0.5800
Epoch 4/5
100/100 [==============================] - 10s 94ms/step - loss: 0.6502 - accuracy: 0.6400 - val_loss: 0.6500 - val_accuracy: 0.6000
Epoch 5/5
100/100 [==============================] - 10s 94ms/step - loss: 0.6351 - accuracy: 0.6620 - val_loss: 0.6385 - val_accuracy: 0.6200



history1 = model.fit(
    train,
    epochs=1,
    validation_data=val,
    verbose=1
)

output:
Epoch 1/1
100/100 [==============================] - 10s 94ms/step - loss: 0.6905 - accuracy: 0.5300 - val_loss: 0.6882 - val_accuracy: 0.5450
